380-600 回合：380/600，奖励：-307.64，评估奖励：-69.95，最佳评估奖励：-69.95，更新模型！ 0.05
        3: -95.77 19.55
        2: -86.78 20.05
540-600 回合：540/600，奖励：-294.96，评估奖励：-69.30，最佳评估奖励：-69.30，更新模型！ 0.05
再次用best_model训练
60-600 回合：60/600，奖励：-227.77，评估奖励：-67.88，最佳评估奖励：-67.88，更新模型！ 0.05
        3: -76.26 21.30
        2: -93.19 24.26
100-600 回合：100/600，奖励：-345.30，评估奖励：-34.74，最佳评估奖励：-34.74，更新模型！ 0.05
        self.actor_lr = 0.0003 # actor网络的学习率
        self.critic_lr = 0.00075 # critic网络的学习率


class Config:
     def __init__(self) -> None:
        self.env_name = "LunarLander-v2" # 环境名字
        self.new_step_api = False # 是否用gym的新api
        self.algo_name = "PPO" # 算法名字
        self.mode = "train" # train or test
        self.seed = 1234 # 随机种子
        self.device = "cuda" # device to use
        self.train_eps = 600 # 训练的回合数
        self.test_eps = 20 # 测试的回合数
        self.max_steps = 200 # 每个回合的最大步数
        self.eval_eps = 5 # 评估的回合数
        self.eval_per_episode = 20 # 评估的频率
        self.epsilon_start = 0.9 # epsilon初始值
        self.epsilon_end = 0.05 # epsilon最终值
        self.epsilon_decay = 500 # epsilon衰减率

        self.train_batch_size = 3 # 每次训练前收集多少轮数的数据
        self.gamma = 0.99 # 折扣因子
        self.k_epochs = 2 # 更新策略网络的次数
        self.actor_lr = 0.0003 # actor网络的学习率
        self.critic_lr = 0.0008 # critic网络的学习率
        self.eps_clip = 0.2 # epsilon-clip
        self.entropy_coef = 0.1 # entropy的系数
        self.update_freq = 100 # 更新频率
        self.actor_hidden_dim = 256 # actor网络的隐藏层维度
        self.critic_hidden_dim = 64 # critic网络的隐藏层维度
        self.input_dim = 256
        self.gama_a_s = 0.25
        self.atten_std = 1
        self.atten_noise = True
        self.mid_save = False
