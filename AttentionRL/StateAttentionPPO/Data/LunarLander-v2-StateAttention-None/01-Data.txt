Ex1.pt 回合：280/600，奖励：-168.70，评估奖励：-66.37，最佳评估奖励：-66.37，更新模型！ 0.10
   3: -89.70 29.23
   2: -121.37 62.57

Ex2.pt 回合：480/600，奖励：-239.27，评估奖励：-36.94，最佳评估奖励：-36.94，更新模型！ 0.05
class Config:
     def __init__(self) -> None:
        self.env_name = "LunarLander-v2" # 环境名字
        self.new_step_api = False # 是否用gym的新api
        self.algo_name = "PPO" # 算法名字
        self.mode = "train" # train or test
        self.seed = 1234 # 随机种子
        self.device = "cuda" # device to use
        self.train_eps = 600 # 训练的回合数
        self.test_eps = 20 # 测试的回合数
        self.max_steps = 200 # 每个回合的最大步数
        self.eval_eps = 5 # 评估的回合数
        self.eval_per_episode = 20 # 评估的频率
        self.epsilon_start = 0.9 # epsilon初始值
        self.epsilon_end = 0.05 # epsilon最终值
        self.epsilon_decay = 500 # epsilon衰减率

        self.train_batch_size = 3 # 每次训练前收集多少轮数的数据
        self.gamma = 0.99 # 折扣因子
        self.k_epochs = 2 # 更新策略网络的次数
        self.actor_lr = 0.0003 # actor网络的学习率
        self.critic_lr = 0.001 # critic网络的学习率
        self.eps_clip = 0.2 # epsilon-clip
        self.entropy_coef = 0.1 # entropy的系数
        self.update_freq = 100 # 更新频率
        self.actor_hidden_dim = 256 # actor网络的隐藏层维度
        self.critic_hidden_dim = 64 # critic网络的隐藏层维度
        self.input_dim = 256
        self.gama_a_s = 0.25
        self.atten_std = 1
        self.atten_noise = False
        self.mid_save = False

   

Ex3.pt: 回合：560/600，奖励：-61.21，评估奖励：-38.27，最佳评估奖励：-38.27，更新模型！ 0.05
class Config:
     def __init__(self) -> None:
        self.env_name = "LunarLander-v2" # 环境名字
        self.new_step_api = False # 是否用gym的新api
        self.algo_name = "PPO" # 算法名字
        self.mode = "train" # train or test
        self.seed = 12345 # 随机种子
        self.device = "cuda" # device to use
        self.train_eps = 600 # 训练的回合数
        self.test_eps = 20 # 测试的回合数
        self.max_steps = 200 # 每个回合的最大步数
        self.eval_eps = 5 # 评估的回合数
        self.eval_per_episode = 20 # 评估的频率
        self.epsilon_start = 0.9 # epsilon初始值
        self.epsilon_end = 0.05 # epsilon最终值
        self.epsilon_decay = 500 # epsilon衰减率

        self.train_batch_size = 3 # 每次训练前收集多少轮数的数据
        self.gamma = 0.99 # 折扣因子
        self.k_epochs = 2 # 更新策略网络的次数
        self.actor_lr = 0.0004 # actor网络的学习率
        self.critic_lr = 0.00075 # critic网络的学习率
        self.eps_clip = 0.2 # epsilon-clip
        self.entropy_coef = 0.1 # entropy的系数
        self.update_freq = 100 # 更新频率
        self.actor_hidden_dim = 256 # actor网络的隐藏层维度
        self.critic_hidden_dim = 64 # critic网络的隐藏层维度
        self.input_dim = 256
        self.gama_a_s = 0.25
        self.atten_std = 1
        self.atten_noise = False
        self.mid_save = False