Ex1.pt 回合：600/600，奖励：572.00，评估奖励：0.00，最佳评估奖励：352.00，1818.05 0.05
Ex1_Best.pt 回合：140/600，奖励：528.00，评估奖励：352.00，最佳评估奖励：352.00，更新模型！1760.05 0.05


进行一次600回合训练之后，再次用agent训练过后的数据
class Config:
     def __init__(self) -> None:
        self.env_name = "BeamRider-v0" # 环境名字
        self.new_step_api = False # 是否用gym的新api
        self.algo_name = "PPO" # 算法名字
        self.mode = "train" # train or test
        self.seed = 1234 # 随机种子
        self.device = "cuda" # device to use
        self.train_eps = 600 # 训练的回合数
        self.test_eps = 10 # 测试的回合数
        self.max_steps = 2000 # 每个回合的最大步数
        self.eval_eps = 1 # 评估的回合数
        self.eval_per_episode = 20 # 评估的频率
        self.epsilon_start = 0.9 # epsilon初始值
        self.epsilon_end = 0.05 # epsilon最终值
        self.epsilon_decay = 1000 # epsilon衰减率

        self.train_batch_size = 3 # 每次训练前收集多少轮数的数据
        self.gamma = 0.9 # 折扣因子
        self.k_epochs = 1 # 更新策略网络的次数
        self.actor_lr = 0.00007 # actor网络的学习率
        self.critic_lr = 0.00003 # critic网络的学习率
        self.eps_clip = 0.2 # epsilon-clip
        self.entropy_coef = 0.05 # entropy的系数
        self.update_freq = 100 # 更新频率
        self.actor_hidden_dim = 256 # actor网络的隐藏层维度
        self.critic_hidden_dim = 64 # critic网络的隐藏层维度
        self.input_dim = 256
        self.gama_a_s = 0.25
        self.atten_std = 1
        self.atten_noise = False
        self.mid_save = False