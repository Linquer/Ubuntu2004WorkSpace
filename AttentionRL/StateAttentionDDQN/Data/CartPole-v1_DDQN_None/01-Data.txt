class Config:
    def __init__(self):
        self.algo_name = 'DoubleDQN' # 算法名称
        self.env_name = 'CartPole-v1' # 环境名称
        self.seed = 1234 # 随机种子
        self.train_eps = 200 # 训练回合数
        self.test_eps = 20  # 测试回合数
        self.max_steps = 200 # 每回合最大步数
        self.eval_per_episode = 10 # 评估频率
        self.eval_eps = 10 # 测试回合数
        self.gamma = 0.99 # 折扣因子
        self.lr = 0.0005 # 学习率
        self.epsilon_start = 0.95 # epsilon初始值
        self.epsilon_end = 0.05 # epsilon最终值
        self.epsilon_decay = 500 # epsilon衰减率
        self.buffer_size = 1000 # ReplayBuffer容量
        self.batch_size = 128 # ReplayBuffer中批次大小
        self.target_update = 4 # 目标网络更新频率
        self.hidden_dim = 256 # 神经网络隐藏层维度
        self.atten_noise = False
        self.mid_save = False
        self.update_freq = 2
        if torch.cuda.is_available(): # 是否使用GPUs
            self.device = 'cuda'
        else:
            self.device = 'cpu'

回合：10/200，奖励：12.00，评估奖励：9.90，最佳评估奖励：9.90，更新模型！ 0.65
回合：20/200，奖励：12.00，评估奖励：9.60，最佳评估奖励：9.90，0.48
回合：30/200，奖励：14.00，评估奖励：9.60，最佳评估奖励：9.90，0.38
回合：40/200，奖励：37.00，评估奖励：9.60，最佳评估奖励：9.90，0.29
回合：50/200，奖励：10.00，评估奖励：9.10，最佳评估奖励：9.90，0.24
回合：60/200，奖励：10.00，评估奖励：10.40，最佳评估奖励：10.40，更新模型！ 0.20
回合：70/200，奖励：22.00，评估奖励：11.60，最佳评估奖励：11.60，更新模型！ 0.17
回合：80/200，奖励：12.00，评估奖励：10.00，最佳评估奖励：11.60，0.14
回合：90/200，奖励：12.00，评估奖励：9.30，最佳评估奖励：11.60，0.11
回合：100/200，奖励：11.00，评估奖励：9.30，最佳评估奖励：11.60，0.10
回合：110/200，奖励：25.00，评估奖励：9.60，最佳评估奖励：11.60，0.09
回合：120/200，奖励：38.00，评估奖励：14.10，最佳评估奖励：14.10，更新模型！ 0.08
回合：130/200，奖励：53.00，评估奖励：28.70，最佳评估奖励：28.70，更新模型！ 0.06
回合：140/200，奖励：48.00，评估奖励：62.70，最佳评估奖励：62.70，更新模型！ 0.05
回合：150/200，奖励：83.00，评估奖励：200.00，最佳评估奖励：200.00，更新模型！ 0.05
回合：160/200，奖励：200.00，评估奖励：200.00，最佳评估奖励：200.00，更新模型！ 0.05
回合：170/200，奖励：200.00，评估奖励：200.00，最佳评估奖励：200.00，更新模型！ 0.05
回合：180/200，奖励：200.00，评估奖励：200.00，最佳评估奖励：200.00，更新模型！ 0.05
回合：190/200，奖励：200.00，评估奖励：200.00，最佳评估奖励：200.00，更新模型！ 0.05
回合：200/200，奖励：200.00，评估奖励：200.00，最佳评估奖励：200.00，更新模型！ 0.05